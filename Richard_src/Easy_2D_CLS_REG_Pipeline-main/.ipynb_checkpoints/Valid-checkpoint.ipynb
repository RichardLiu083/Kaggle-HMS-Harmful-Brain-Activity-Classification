{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c42fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "import  matplotlib.pyplot as plt\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "from pytorch_toolbelt import losses as L\n",
    "\n",
    "# Utils\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## using gpu:1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b494caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WidthAttention(nn.Module):\n",
    "    def __init__(self, in_ch, width: int):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = self.attention(x)\n",
    "        attention = attention.unsqueeze(1).unsqueeze(1)\n",
    "        return x * attention\n",
    "\n",
    "class Customize_Model(nn.Module):\n",
    "    def __init__(self, model_name, cls):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, image):\n",
    "        x = self.model(image)\n",
    "        return x\n",
    "\n",
    "class Slide_Window_Model(nn.Module):\n",
    "    def __init__(self, model_name, cls):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, image):\n",
    "        x = self.model(image)  ## (1,1280,13,21)\n",
    "        x = self.att(x)  ## (1,1280,13,21)\n",
    "        x = self.gp(x)   ## (1,1280)\n",
    "        x = self.out(x)  ## (1,6)\n",
    "        return x if self.training else x.view(-1, self.cls, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad1498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import spectrogram_from_eeg\n",
    "\n",
    "def get_test_transform(img_size):\n",
    "    return A.Compose([\n",
    "        A.PadIfNeeded(min_height=400, min_width=CFG['img_crop'], border_mode=0, p=1),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ])\n",
    "\n",
    "class Customize_Dataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def read_data(self, data):\n",
    "        def norm_to_255(img):\n",
    "            img= img-img.min()\n",
    "            img= img/img.max()\n",
    "            img= img*255\n",
    "            return img.astype(np.uint8)\n",
    "        \n",
    "        def norm_to_standard(img):\n",
    "            ep = 1e-6\n",
    "            m = np.nanmean(img.flatten())\n",
    "            s = np.nanstd(img.flatten())\n",
    "            img = (img-m)/(s+ep)\n",
    "            img = np.nan_to_num(img, nan=0.0)\n",
    "            return img\n",
    "        \n",
    "        ## train_spectrograms\n",
    "        path= data['image_path'].replace('train_eegs','train_spectrograms')\n",
    "        if 'test_eegs' in data['image_path']: path= data['image_path'].replace('test_eegs','test_spectrograms')\n",
    "        path= path.replace(str(data['eeg_id']), str(data['spectrogram_id']))\n",
    "        raw= pd.read_parquet(path).fillna(0)\n",
    "        \n",
    "        col= list(raw.filter(like='LL', axis=1))\n",
    "        img_LL= np.log1p(raw[col].T.values)\n",
    "        col= list(raw.filter(like='RL', axis=1))\n",
    "        img_RL= np.log1p(raw[col].T.values)\n",
    "        col= list(raw.filter(like='RP', axis=1))\n",
    "        img_RP= np.log1p(raw[col].T.values)\n",
    "        col= list(raw.filter(like='LP', axis=1))\n",
    "        img_LP= np.log1p(raw[col].T.values)\n",
    "        \n",
    "        img= np.concatenate([img_LL, img_LP, img_RP, img_RL], axis=0)\n",
    "        img= np.expand_dims(img, axis=2)\n",
    "        img= np.concatenate([img, img, img], axis=2)\n",
    "        img_spectrograms= norm_to_standard(img)\n",
    "        \n",
    "        ## train_eegs\n",
    "        img_10= spectrogram_from_eeg(data['image_path'], duration=10, height=100)\n",
    "        img_10= np.concatenate([img_10[..., 0],\n",
    "                                img_10[..., 1],\n",
    "                                img_10[..., 2],\n",
    "                                img_10[..., 3]], axis=0)\n",
    "        img_30= spectrogram_from_eeg(data['image_path'], duration=30, height=100)\n",
    "        img_30= np.concatenate([img_30[..., 0],\n",
    "                                img_30[..., 1],\n",
    "                                img_30[..., 2],\n",
    "                                img_30[..., 3]], axis=0)\n",
    "        img= np.concatenate([img_10, img_30], axis=1)\n",
    "        img= np.expand_dims(img, axis=2)\n",
    "        img= np.concatenate([img, img, img], axis=2)\n",
    "        img_eeg= img\n",
    "        \n",
    "        ## fuse img\n",
    "        img_spectrograms= img_spectrograms[:, :, :1]\n",
    "        img_eeg= img_eeg[..., :1]\n",
    "        img= np.concatenate([img_eeg, img_spectrograms], axis=1)\n",
    "        img= np.concatenate([img, img, img], axis=2)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.df.loc[index]\n",
    "        img= self.read_data(data)\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "        return {\n",
    "            'image': torch.tensor(img, dtype=torch.float32),\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add9004c",
   "metadata": {},
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ee22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG= {\n",
    "    'fold': 0,\n",
    "    'img_size': None,\n",
    "    'img_crop': 656,\n",
    "    'TTA': 1,\n",
    "    'stage': 1,\n",
    "    \n",
    "    'pseudo_label': False,\n",
    "    'model': [\n",
    "        './train_model/cv0_best.ts',\n",
    "        \n",
    "#         './test_model/effb0_lb34/cv0_best.ts',\n",
    "    ],\n",
    "}\n",
    "CFG['model']= [ torch.load(m, map_location= 'cuda:0') for m in CFG['model'] ]\n",
    "print(f\"length of model: {len(CFG['model'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0061056",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ecfbd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('../Data/train_eeg.csv')\n",
    "if CFG['stage']==2: df= df[df['voter']>7]\n",
    "if CFG['pseudo_label']: df['fold']= CFG['fold']\n",
    "else: df= df.drop_duplicates(subset=['spectrogram_id'])\n",
    "\n",
    "train_df= df[df['fold']!=CFG['fold']].reset_index(drop=True)\n",
    "valid_df= df[df['fold']==CFG['fold']].reset_index(drop=True)\n",
    "print(f'train dataset: {len(train_df)}')\n",
    "print(f'valid dataset: {len(valid_df)}')\n",
    "\n",
    "valid_dataset= Customize_Dataset(valid_df, get_test_transform(CFG['img_size']))\n",
    "valid_loader= DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f300948e",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data= valid_dataset[0]\n",
    "img= data['image']\n",
    "print(img.shape)\n",
    "plt.imshow(img.permute(1,2,0).numpy()[...,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca9943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "def slide_inference(model, img):\n",
    "    \n",
    "    img= torch.unsqueeze(img, 0).cuda()\n",
    "    for i, m in enumerate(model):\n",
    "        with torch.no_grad():\n",
    "            m.eval()\n",
    "            \n",
    "            imgs= torch.cat([img, img.flip(-1), img.flip(-2), img.flip(-1).flip(-2)], dim=0)\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                pred= sliding_window_inference(imgs[:CFG['TTA']],\n",
    "                                                roi_size=(-1,CFG['img_crop']), \n",
    "                                                mode= 'gaussian',\n",
    "                                                sw_batch_size=4, \n",
    "                                                overlap=0.25,\n",
    "                                                predictor=m)\n",
    "                pred= pred.view(pred.shape[0],6,-1).mean(dim=-1)\n",
    "                pred= pred.mean(dim=0)\n",
    "                \n",
    "        if i==0: preds= pred.softmax(dim=-1)\n",
    "        else: preds+= pred.softmax(dim=-1)\n",
    "        \n",
    "    preds= preds/len(model)\n",
    "    preds= preds.cpu().numpy()\n",
    "    return preds\n",
    "\n",
    "def inference(model, img):\n",
    "    \n",
    "    img= torch.unsqueeze(img, 0).cuda()\n",
    "    for i, m in enumerate(model):\n",
    "        with torch.no_grad():\n",
    "            m.eval()\n",
    "            \n",
    "            imgs= torch.cat([img, img.flip(-1), img.flip(-2), img.flip(-1).flip(-2)], dim=0)\n",
    "            with torch.no_grad():\n",
    "                pred= m(imgs[:CFG['TTA']])\n",
    "                pred= pred.mean(dim=0)\n",
    "                \n",
    "        if i==0: preds= pred.softmax(dim=-1)\n",
    "        else: preds+= pred.softmax(dim=-1)\n",
    "            \n",
    "    preds= preds/len(model)\n",
    "    preds= preds.cpu().numpy()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79354f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_df['pred_cls']= None\n",
    "count= 0\n",
    "for i, data in enumerate(tqdm(valid_loader)):\n",
    "    for j in range(len(data['image'])):\n",
    "        img= data['image'][j]\n",
    "        prob= slide_inference(CFG['model'], img)\n",
    "        \n",
    "        valid_df.loc[count, 'pred_cls']= prob.argmax(0)\n",
    "        valid_df.loc[count, 'prob']= str(prob.tolist())\n",
    "        count+= 1\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a56af7",
   "metadata": {},
   "source": [
    "# Pseudo Label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG['pseudo_label']:\n",
    "    df= pd.read_csv('../Data/train_npy_PL1.csv')\n",
    "    df[f\"PL_prob_cv{CFG['fold']}\"]= valid_df['prob']\n",
    "    df.to_csv('../Data/train_npy_PL1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0831d24",
   "metadata": {},
   "source": [
    "# Confusion_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f471c75b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn. metrics import roc_auc_score\n",
    "\n",
    "cm_df= pd.DataFrame( confusion_matrix(valid_df['label'].values, \n",
    "                                      valid_df['pred_cls'].astype(np.int64).values) )\n",
    "print(f'row: label, column: pred')\n",
    "\n",
    "\n",
    "for i in range(len(cm_df)):\n",
    "    cm_df.loc[i,'recall']= cm_df.loc[i,i] / cm_df.loc[i].sum()\n",
    "    cm_df.loc[i,'precision']= cm_df.loc[i,i] / cm_df[i].sum()\n",
    "recall= cm_df.loc[1,'recall']\n",
    "precision= cm_df.loc[1,'precision']\n",
    "f1_score= (2*recall*precision)/(recall+precision)\n",
    "print(f'f1_score: {f1_score}')\n",
    "print(f\"mean recall: {cm_df['recall'].mean()}\")\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab7610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "\n",
    "pred= []\n",
    "soft_label= []\n",
    "for i in range(len(valid_df)):\n",
    "    p= eval(valid_df.loc[i, 'prob'])\n",
    "    l= eval(valid_df.loc[i, 'soft_label'])\n",
    "    pred.append(p)\n",
    "    soft_label.append(l)\n",
    "pred= np.array(pred)\n",
    "soft_label= np.array(soft_label)\n",
    "\n",
    "kl_divergence(soft_label, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd6c39",
   "metadata": {},
   "source": [
    "# Grad_Cam"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bca3ef08",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reshape_transform(tensor, height=7, width=7):\n",
    "    result = tensor.reshape(tensor.size(0),\n",
    "                            height, width, tensor.size(2))\n",
    "\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result\n",
    "\n",
    "\n",
    "data= valid_dataset[10]\n",
    "img= data['image']\n",
    "rgb_img= img.permute(1,2,0).numpy()\n",
    "\n",
    "target_layers = [model.model.conv_head]\n",
    "cam= GradCAM(model=model, \n",
    "             target_layers=target_layers, \n",
    "#              reshape_transform=reshape_transform, ## if swin_tranformer\n",
    "             use_cuda=True)\n",
    "\n",
    "input_tensor = img.unsqueeze(0)\n",
    "targets = [ClassifierOutputTarget(0)]\n",
    "grayscale_cam = cam(\n",
    "                    input_tensor=input_tensor, \n",
    "                    eigen_smooth=True,\n",
    "                    aug_smooth=True,\n",
    "                    targets=targets,\n",
    "#                     targets=None,  ## if swin_tranformer\n",
    "                )\n",
    "\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "plt.imshow(visualization)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
