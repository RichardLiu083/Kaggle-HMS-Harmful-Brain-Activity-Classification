{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "from pytorch_toolbelt import losses as L\n",
    "\n",
    "# Utils\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "from timm.utils import ModelEmaV2\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## using gpu:1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ceec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customize_Model(nn.Module):\n",
    "    def __init__(self, model_name, cls):\n",
    "        super().__init__()\n",
    "        self.cls= cls\n",
    "        self.model = timm.create_model(model_name, \n",
    "                                       pretrained=True, \n",
    "                                       num_classes=self.cls, \n",
    "                                       in_chans= 4,\n",
    "                                       drop_rate= CFG['drop_out'], \n",
    "                                       drop_path_rate= CFG['drop_path'])\n",
    "        \n",
    "    def forward(self, image):\n",
    "        x = self.model(image)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Slide_Window_Model(nn.Module):\n",
    "    def __init__(self, model_name, cls):\n",
    "        super().__init__()\n",
    "        self.cls= cls\n",
    "        self.model = timm.create_model(model_name, \n",
    "                                       pretrained=True, \n",
    "                                       num_classes=cls, \n",
    "                                       in_chans= 4,\n",
    "                                       drop_rate= CFG['drop_out'], \n",
    "                                       drop_path_rate= CFG['drop_path'])\n",
    "        \n",
    "    def forward(self, image):\n",
    "        x = self.model(image)\n",
    "        return x if self.training else x.view(-1, self.cls, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform(img_size):\n",
    "    return A.Compose([\n",
    "        A.SmallestMaxSize(max_size=img_size, interpolation=3, p=1),\n",
    "#         A.Resize(img_size, img_size),\n",
    "        A.RandomCrop(CFG['img_crop'], CFG['img_crop'], p=1),\n",
    "        \n",
    "        \n",
    "        A.HorizontalFlip(p=0.5),\n",
    "#         A.VerticalFlip(p=0.5),\n",
    "#         A.Transpose(p=0.5),\n",
    "#         A.RandomRotate90(p=0.5),\n",
    "        \n",
    "#         A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.5),\n",
    "#         A.Blur(blur_limit= 3, p=0.3), \n",
    "#         A.GaussNoise(p=0.3),\n",
    "#         A.OneOf([\n",
    "#             A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n",
    "#             A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n",
    "#         ], p=0.3),\n",
    "        A.ShiftScaleRotate(shift_limit=0.15, scale_limit=0.05, rotate_limit= 15,\n",
    "                                        interpolation=cv2.INTER_LINEAR, border_mode=0, p=0.7),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_test_transform(img_size):\n",
    "    return A.Compose([\n",
    "        A.SmallestMaxSize(max_size=img_size, interpolation=3, p=1),\n",
    "#         A.Resize(img_size, img_size),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a153977",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customize_Dataset(Dataset):\n",
    "    def __init__(self, df, transforms=None, mixup=False):\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "        self.mixup= mixup\n",
    "    \n",
    "    def mixup_aug(self, img_1, mask_1, \n",
    "                        img_2, mask_2):\n",
    "        \"\"\"\n",
    "        img: numpy array of shape (height, width,channel)\n",
    "        mask: numpy array of shape (height, width,channel)\n",
    "        \"\"\"\n",
    "        ## mixup\n",
    "        weight= np.random.beta(a=0.5, b=0.5)\n",
    "        img= img_1*weight + img_2*(1-weight)\n",
    "        mask= mask_1*weight + mask_2*(1-weight)\n",
    "        return img.astype(np.uint8), mask\n",
    "    \n",
    "    def read_data(self, data):\n",
    "        def norm_to_255(img):\n",
    "            img= img-img.min()\n",
    "            img= img/img.max()\n",
    "            img= img*255\n",
    "            return img.astype(np.uint8)\n",
    "        \n",
    "        def norm_to_standard(img):\n",
    "            ep = 1e-6\n",
    "            m = np.nanmean(img.flatten())\n",
    "            s = np.nanstd(img.flatten())\n",
    "            img = (img-m)/(s+ep)\n",
    "            img = np.nan_to_num(img, nan=0.0)\n",
    "            return img\n",
    "        \n",
    "        raw= pd.read_parquet(data['image_path'])\n",
    "        \n",
    "        col= list(raw.filter(like='LL', axis=1))\n",
    "        img_LL= np.log( np.clip(raw[col].T.values,np.exp(-4),np.exp(8)) )\n",
    "        col= list(raw.filter(like='RL', axis=1))\n",
    "        img_RL= np.log( np.clip(raw[col].T.values,np.exp(-4),np.exp(8)) )\n",
    "        col= list(raw.filter(like='RP', axis=1))\n",
    "        img_RP= np.log( np.clip(raw[col].T.values,np.exp(-4),np.exp(8)) )\n",
    "        col= list(raw.filter(like='LP', axis=1))\n",
    "        img_LP= np.log( np.clip(raw[col].T.values,np.exp(-4),np.exp(8)) )\n",
    "        \n",
    "        img= np.array([img_LL, img_RL, img_RP, img_LP]).transpose(1,2,0)\n",
    "        img= norm_to_standard(img)\n",
    "        \n",
    "        label= np.array(eval(data['soft_label']))\n",
    "        return img, label\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.df.loc[index]\n",
    "        img, label= self.read_data(data)\n",
    "        \n",
    "        # use mixup\n",
    "        if self.mixup and np.random.rand() >= (1-self.mixup):\n",
    "            img_1= img\n",
    "            label_1= np.array(label)\n",
    "            while True:\n",
    "                indx= np.random.randint(len(self.df))\n",
    "                data= self.df.loc[indx]\n",
    "                img_2, label_2= self.read_data(data)\n",
    "                if label_1.argmax(0)!=label_2.argmax(0): break\n",
    "            img, label= self.mixup_aug(img_1, label_1, \n",
    "                                       img_2, label_2)\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "        return {\n",
    "            'image': torch.tensor(img, dtype=torch.float32),\n",
    "            'label': torch.tensor(label, dtype=torch.float32),\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e62a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customize_loss(nn.Module):\n",
    "    def  __init__(self):\n",
    "        super().__init__()\n",
    "        self.CrossEntropy= nn.CrossEntropyLoss(weight= None, label_smoothing=0.)\n",
    "        self.FocalCosineLoss= L.FocalCosineLoss()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss= 1*self.CrossEntropy(y_pred, y_true)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f51ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, criterion, optimizer, model_ema):\n",
    "    scaler= amp.GradScaler()\n",
    "    model.train()\n",
    "\n",
    "    ep_loss= []\n",
    "    for i, data in enumerate(tqdm(dataloader)):\n",
    "\n",
    "        imgs= data['image'].to('cuda')\n",
    "        labels= data['label'].to('cuda')\n",
    "        \n",
    "        with amp.autocast():\n",
    "            preds= model(imgs)\n",
    "            loss= criterion(preds, labels)\n",
    "            ep_loss.append(loss.item())\n",
    "            loss/= CFG['gradient_accumulation']\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i+1) % CFG['gradient_accumulation']== 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            if model_ema: model_ema.update(model)\n",
    "                \n",
    "    return np.mean(ep_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efbece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "\n",
    "# def valid_epoch(dataloader, model, criterion):\n",
    "#     model.eval()\n",
    "    \n",
    "#     ep_loss= []\n",
    "#     all_pred= []\n",
    "#     all_label= []\n",
    "#     for i, data in enumerate(tqdm(dataloader)):\n",
    "\n",
    "#         imgs= data['image'].to('cuda')\n",
    "#         labels= data['label'].to('cuda')\n",
    "#         all_label.extend(labels.cpu().numpy())\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             preds= model(imgs)\n",
    "#             loss= criterion(preds, labels)\n",
    "#             ep_loss.append(loss.item())\n",
    "#         all_pred.extend(preds.cpu().softmax(dim=-1).numpy())\n",
    "        \n",
    "    \n",
    "#     ## caculate metrics\n",
    "#     all_label= np.array(all_label).argmax(1)\n",
    "#     all_pred= np.array(all_pred)\n",
    "    \n",
    "#     acc= Accuracy(all_pred, all_label)\n",
    "#     print(f'accuracy: {acc}')\n",
    "#     recall= Mean_Recall(all_pred, all_label)\n",
    "#     print(f'mean_recall: {recall}')\n",
    "#     auc= AUC(all_pred, all_label)\n",
    "#     print(f'AUC: {auc}')\n",
    "    \n",
    "#     score= auc\n",
    "#     return np.mean(ep_loss), score\n",
    "\n",
    "\n",
    "def valid_epoch(dataloader, model, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    ep_loss= []\n",
    "    all_pred= []\n",
    "    all_label= []\n",
    "    for i, data in enumerate(tqdm(dataloader)):\n",
    "\n",
    "        imgs= data['image'].to('cuda')\n",
    "        labels= data['label'].to('cuda')\n",
    "        all_label.extend(labels.cpu().numpy())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds= sliding_window_inference(imgs, \n",
    "                                            roi_size=CFG['img_crop'], \n",
    "                                            mode= 'gaussian',\n",
    "                                            sw_batch_size=4, \n",
    "                                            predictor=model)\n",
    "            preds= preds.view(preds.shape[0],model.cls,-1).max(dim=-1).values\n",
    "            loss= criterion(preds, labels)\n",
    "            ep_loss.append(loss.item())\n",
    "        all_pred.extend(preds.cpu().softmax(dim=-1).numpy())\n",
    "    \n",
    "    ## caculate metrics\n",
    "    soft_label= all_label.copy()\n",
    "    all_label= np.array(all_label).argmax(1)\n",
    "    all_pred= np.array(all_pred)\n",
    "    \n",
    "    acc= Accuracy(all_pred, all_label)\n",
    "    print(f'accuracy: {acc}')\n",
    "    recall= Mean_Recall(all_pred, all_label)\n",
    "    print(f'mean_recall: {recall}')\n",
    "    auc= AUC(all_pred, all_label)\n",
    "    print(f'AUC: {auc}')\n",
    "    kl_score= kl_divergence(soft_label, all_pred)\n",
    "    print(f'kl_divergence: {kl_score}')\n",
    "    \n",
    "    score= kl_score\n",
    "    return np.mean(ep_loss), score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bde5f",
   "metadata": {},
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fba4cb79",
   "metadata": {},
   "source": [
    "timm.list_models(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8029a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG= {\n",
    "    'fold': 0,\n",
    "    'epoch': 30,\n",
    "    'model_name': 'tf_efficientnet_b0_ns',\n",
    "    \n",
    "    'img_size': 224,\n",
    "    'img_crop': 224,\n",
    "    \n",
    "    'batch_size': 32,\n",
    "    'gradient_accumulation': 1,\n",
    "    'gradient_checkpoint': False,\n",
    "    'drop_out': 0.3,\n",
    "    'drop_path': 0.2,\n",
    "    'mixup': 0.,\n",
    "    'EMA': 0., #0.995\n",
    "    \n",
    "    'lr': 3e-4,\n",
    "    'weight_decay': 0.,\n",
    "    \n",
    "    'num_classes': 6,\n",
    "    'load_model': False,\n",
    "    'save_model': './train_model'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aa7835",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0010c7a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('../Data/train_spectrogram.csv')\n",
    "train_df= df[df['fold']!=CFG['fold']].reset_index(drop=True)\n",
    "valid_df= df[df['fold']==CFG['fold']].reset_index(drop=True)\n",
    "print(f'train dataset: {len(train_df)}')\n",
    "print(f'valid dataset: {len(valid_df)}')\n",
    "\n",
    "train_dataset= Customize_Dataset(train_df, get_train_transform(CFG['img_size']), mixup=CFG['mixup'])\n",
    "valid_dataset= Customize_Dataset(valid_df, get_test_transform(CFG['img_size']), mixup=False)\n",
    "\n",
    "train_loader= DataLoader(train_dataset, batch_size= CFG['batch_size'], shuffle=True, num_workers=0)\n",
    "valid_loader= DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5c80f34",
   "metadata": {},
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "cls_weight= compute_class_weight(class_weight='balanced', \n",
    "                                 classes=list(range(CFG['num_classes'])), \n",
    "                                 y=train_df['label'].values)\n",
    "cls_weight= torch.tensor(cls_weight).cuda()\n",
    "cls_weight"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75020c52",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data= valid_dataset[0]\n",
    "img= data['image']\n",
    "label= data['label']\n",
    "print(label)\n",
    "plt.imshow(img.permute(1,2,0).numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd8144c",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce46fc57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create model\n",
    "if CFG['load_model']:\n",
    "    print(f\"load_model: {CFG['load_model']}\")\n",
    "    model= torch.load(CFG['load_model'], map_location= 'cuda')\n",
    "else:\n",
    "    model= Slide_Window_Model(CFG['model_name'], CFG['num_classes'])\n",
    "    \n",
    "if CFG['gradient_checkpoint']: \n",
    "    print('use gradient checkpoint')\n",
    "    model.model.set_grad_checkpointing(enable=True)\n",
    "    \n",
    "## EMA\n",
    "model.to('cuda')\n",
    "if CFG['EMA']:\n",
    "    print(f\"Use EMA: {CFG['EMA']}\")\n",
    "    model_ema= ModelEmaV2(model, decay=CFG['EMA'])\n",
    "    model_ema.to('cuda')\n",
    "else:\n",
    "    model_ema= type('model_ema', (object,), {'module':{}})\n",
    "    \n",
    "## hyperparameter\n",
    "criterion= Customize_loss()\n",
    "optimizer= optim.AdamW(model.parameters(), lr= CFG['lr'], weight_decay= CFG['weight_decay'])\n",
    "\n",
    "## start training\n",
    "best_score= 100000\n",
    "for ep in range(1, CFG['epoch']+1):\n",
    "    print(f'\\nep: {ep}')\n",
    "    \n",
    "    if CFG['EMA']: train_loss= train_epoch(train_loader, model, criterion, optimizer, model_ema)\n",
    "    else: \n",
    "        train_loss= train_epoch(train_loader, model, criterion, optimizer, False)\n",
    "        model_ema.module= model\n",
    "    valid_loss, valid_acc= valid_epoch(valid_loader, model_ema.module, criterion)\n",
    "    print(f'train loss: {round(train_loss, 5)}')\n",
    "    print(f'valid loss: {round(valid_loss, 5)}, valid_acc: {round(valid_acc, 5)}')\n",
    "    \n",
    "    if valid_acc <= best_score:\n",
    "        best_score= valid_acc\n",
    "        torch.save(model_ema.module, f\"{CFG['save_model']}/cv{CFG['fold']}_best.pth\")\n",
    "        print(f'model save at score: {round(best_score, 5)}')\n",
    "        \n",
    "        ## save model every epoch\n",
    "        torch.save(model_ema.module, f\"{CFG['save_model']}/cv{CFG['fold']}_ep{ep}.pth\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ce60668",
   "metadata": {},
   "source": [
    "effb0:\n",
    "imgsz= small_224, crop=224, hard_label, no_aug, cv=0.88, lb=0.77\n",
    "imgsz= small_224, crop=224, soft_label, no_aug, cv=0.83\n",
    "imgsz= small_224, crop=224, soft_label, no_aug, cls_weight, cv=1.01\n",
    "imgsz= square_224, crop=224, soft_label, no_aug, cv=0.915\n",
    "imgsz= small_224, crop=224, soft_label, weak_aug, cv=0.829, lb=0.72\n",
    "                                                  cv=0.806\n",
    "imgsz= small_224, crop=224, soft_label, weak_aug, channel_norm, cv=0.873\n",
    "imgsz= small_224, crop=224, soft_label, weak_aug, standard_norm, cv=0.77, lb=0.69\n",
    "\n",
    "\n",
    "full_crop\n",
    "sigmoid\n",
    "channel\n",
    "kl_loss = nn.KLDivLoss(reduction=\"batchmean\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
