{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-07T11:36:32.15643Z",
     "iopub.status.busy": "2024-04-07T11:36:32.155989Z",
     "iopub.status.idle": "2024-04-07T11:36:56.536077Z",
     "shell.execute_reply": "2024-04-07T11:36:56.53448Z",
     "shell.execute_reply.started": "2024-04-07T11:36:32.156394Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pywt\n",
    "from librosa.feature import melspectrogram\n",
    "from librosa import power_to_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T11:36:56.538991Z",
     "iopub.status.busy": "2024-04-07T11:36:56.538414Z",
     "iopub.status.idle": "2024-04-07T11:36:57.677502Z",
     "shell.execute_reply": "2024-04-07T11:36:57.676128Z",
     "shell.execute_reply.started": "2024-04-07T11:36:56.538951Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls -al ../input/hms-harmful-brain-activity-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T11:36:57.680982Z",
     "iopub.status.busy": "2024-04-07T11:36:57.680438Z",
     "iopub.status.idle": "2024-04-07T11:37:34.609555Z",
     "shell.execute_reply": "2024-04-07T11:37:34.607877Z",
     "shell.execute_reply.started": "2024-04-07T11:36:57.680924Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install /kaggle/input/pip-whl/einops-0.7.0-py3-none-any.whl \\\n",
    "    /kaggle/input/pip-whl/beartype-0.18.2-py3-none-any.whl \\\n",
    "    /kaggle/input/pip-whl/rotary_embedding_torch-0.5.3-py3-none-any.whl \\\n",
    "    --default-timeout 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T11:37:34.61669Z",
     "iopub.status.busy": "2024-04-07T11:37:34.616214Z",
     "iopub.status.idle": "2024-04-07T11:37:34.691335Z",
     "shell.execute_reply": "2024-04-07T11:37:34.689775Z",
     "shell.execute_reply.started": "2024-04-07T11:37:34.616653Z"
    }
   },
   "outputs": [],
   "source": [
    "class WidthAttention(nn.Module):\n",
    "    def __init__(self, in_ch, width: int, debug_mode=False):\n",
    "        super().__init__()\n",
    "        h_dim = 64\n",
    "        self.attention = nn.Sequential(  # B, w\n",
    "            nn.Conv2d(in_ch, h_dim, kernel_size=(1, 1)),\n",
    "            nn.BatchNorm2d(h_dim),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(h_dim, width),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.feat_atten = None\n",
    "        if not debug_mode:\n",
    "            self.attention.register_forward_hook(self._capture_attention)\n",
    "\n",
    "    def _capture_attention(self, module, input, output):\n",
    "        self.feat_atten = output\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = self.attention(x)\n",
    "        attention = attention.unsqueeze(1).unsqueeze(1)\n",
    "        return x * attention\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, in_ch, width: int, heads: int, debug_mode=False):\n",
    "        super().__init__()\n",
    "        self.attentions = nn.ModuleList([WidthAttention(in_ch // (heads * 2), width, debug_mode) for _ in range(heads)])\n",
    "        assert in_ch % heads == 0, f'in_ch: {in_ch} must be divisible by heads: {heads}'\n",
    "        self.projections = nn.ModuleList([nn.Conv2d(in_ch, in_ch // (heads * 2), kernel_size=(1, 1)) for _ in range(heads)])\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Conv2d(in_ch // 2, in_ch // 4, kernel_size=(1, 1)),\n",
    "            nn.BatchNorm2d(in_ch // 4),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(in_ch // 4, in_ch // 4, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(in_ch // 4),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(in_ch // 4, in_ch, kernel_size=(1, 1)),\n",
    "            nn.BatchNorm2d(in_ch),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        heads = []\n",
    "        for i, atten in enumerate(self.attentions):\n",
    "            head = self.projections[i](x)\n",
    "            head = atten(head)\n",
    "            heads.append(head)\n",
    "        return self.fuse(torch.cat(heads, 1))\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, width: int, in_ch=4, num_classes=6, weights='IMAGENET1K_V1', use_attention=False, debug_mode=False, cnn_type='b0'):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        if cnn_type == 'b0':\n",
    "            efficientnet = models.efficientnet_b0(weights=weights)\n",
    "            ori_net = list(efficientnet.features.children())\n",
    "            cnn_ch = 1280\n",
    "            c0_ch = 32\n",
    "            w_factor = 5\n",
    "        elif cnn_type == 'v2s':\n",
    "            efficientnet = models.efficientnet_v2_s(weights=weights)\n",
    "            ori_net = list(efficientnet.features.children())\n",
    "            cnn_ch = 1280\n",
    "            c0_ch = 24\n",
    "            w_factor = 5\n",
    "        elif cnn_type == 'convnext-tiny':\n",
    "            network = models.convnext_tiny(weights=weights)\n",
    "            ori_net = list(network.children())[0]\n",
    "            cnn_ch = 768\n",
    "            c0_ch = 96\n",
    "            w_factor = 4\n",
    "        elif cnn_type == 'mobilenet':\n",
    "            network = models.mobilenet_v3_large(weights=weights)\n",
    "            ori_net = list(network.children())[0]\n",
    "            cnn_ch = 960\n",
    "            c0_ch = 16\n",
    "            w_factor = 5\n",
    "        else:\n",
    "            raise NotImplementedError(f'cnn_type: {cnn_type} not implemented')\n",
    "        self.feat_atten = None\n",
    "        if use_attention:\n",
    "            w = width // (2 ** 5)\n",
    "            self.width_attention = MultiHeadAttention(cnn_ch, w + 1, 4, debug_mode)\n",
    "            ori_net.append(self.width_attention)\n",
    "        self.features = nn.Sequential(*ori_net)\n",
    "        # self.features[0][0] = nn.Conv2d(in_ch, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        stride = 2 ** (5 - w_factor + 1)\n",
    "        self.features[0][0] = nn.Conv2d(in_ch, c0_ch, kernel_size=(17, 17), stride=(stride, stride), padding=(8, 8), bias=False)\n",
    "        # self.features[0][0] = nn.Conv2d(in_ch, 32, kernel_size=(11, 11), stride=(2, 2), padding=(6, 6), bias=False)\n",
    "        self.adv_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(cnn_ch, num_classes, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adv_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ElanBlock(nn.Module):\n",
    "    def __init__(self, h_dim, depth=3):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.cv1 = nn.Conv2d(h_dim, h_dim // 2, kernel_size=(1, 1))\n",
    "        self.necks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(h_dim // 4, h_dim // 4, kernel_size=(5, 5), padding=(2, 2), groups=h_dim // 4, bias=False),\n",
    "                nn.BatchNorm2d(h_dim // 4),\n",
    "                nn.GELU(),\n",
    "                nn.Conv2d(h_dim // 4, h_dim // 4, kernel_size=(1, 1), bias=False),\n",
    "                nn.BatchNorm2d(h_dim // 4),\n",
    "            ) for _ in range(depth)\n",
    "        ])\n",
    "        self.cv2 = nn.Sequential(\n",
    "            nn.Conv2d(h_dim // 4 * (1 + depth), h_dim, kernel_size=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(h_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x)\n",
    "        xs = list(torch.chunk(x, 2, 1))\n",
    "        x = xs[-1]\n",
    "        xs.pop()\n",
    "        for neck in self.necks:\n",
    "            x = neck(x)\n",
    "            xs.append(x)\n",
    "        x = self.cv2(torch.cat(xs, 1))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadRotaryAttention(nn.Module):\n",
    "    def __init__(self, h_dim: int, heads: int, is_2d=False):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.h_dim = h_dim\n",
    "        self.is_2d = is_2d\n",
    "        self.rope = rotary_embedding_torch.RotaryEmbedding(dim=h_dim // heads // 2)\n",
    "        if is_2d:\n",
    "            self.projection = nn.Conv2d(h_dim, h_dim, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.projection = nn.Linear(h_dim, h_dim * 3)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.LayerNorm(h_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        xp = self.projection(x)\n",
    "        if self.is_2d:\n",
    "            q, k, v = map(lambda t: rearrange(t, 'b (hs d) h w -> b hs h w d', hs=self.heads), [xp, xp, xp])\n",
    "            q, k = map(self.rope.rotate_queries_or_keys, [q, k])\n",
    "            q, k, v = map(lambda t: rearrange(t, 'b hs h w d -> b hs (h w) d', hs=self.heads), [q, k, v])\n",
    "            attention = torch.einsum('b h i d, b h j d -> b h i j', q, k) * (self.h_dim ** -0.5)\n",
    "            attention = torch.nn.functional.softmax(attention, dim=-1)\n",
    "            xv = torch.einsum('b h i j, b h j c -> b h i c', attention, v)\n",
    "            x = rearrange(xv, 'b h s c -> b s (h c)') + rearrange(x, 'b c h w -> b (h w) c')\n",
    "        else:\n",
    "            q, k, v = xp.chunk(3, dim=-1)\n",
    "            q, k, v = map(lambda t: rearrange(t, 'b s (h d) -> b h s d', h=self.heads), [q, k, v])\n",
    "            q, k = map(self.rope.rotate_queries_or_keys, [q, k])\n",
    "            attention = torch.einsum('b h i d, b h j d -> b h i j', q, k) * (self.h_dim ** -0.5)\n",
    "            attention = torch.nn.functional.softmax(attention, dim=-1)\n",
    "            xv = torch.einsum('b h i j, b h j d -> b h i d', attention, v)\n",
    "            x = rearrange(xv, 'b h s d -> b s (h d)') + x\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class EfficientTransNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_ch=4,\n",
    "                 num_classes=6,\n",
    "                 hid_dim=128,\n",
    "                 weights='IMAGENET1K_V1',\n",
    "                 debug_mode=False,\n",
    "                 width=656 + 256,\n",
    "                 pe_type='rotary',\n",
    "                 use_pe_2d=False,\n",
    "                 heads=8\n",
    "         ):\n",
    "        super(EfficientTransNet, self).__init__()\n",
    "        efficientnet = models.efficientnet_b0(weights=weights)\n",
    "        ori_net = list(efficientnet.features.children())\n",
    "        self.feat_atten = None\n",
    "        self.features = nn.Sequential(*ori_net[:-2])\n",
    "        self.features[0][0] = nn.Conv2d(in_ch, 32, kernel_size=(17, 17), stride=(2, 2), padding=(8, 8), bias=False)\n",
    "        self.cnn_proj = nn.Sequential(\n",
    "            nn.Conv2d(192, hid_dim, kernel_size=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(hid_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        # Transformer related\n",
    "        # Create class token with 2d PE\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, hid_dim), requires_grad=True)\n",
    "        # self.cls_token = nn.Parameter(torch.zeros((1, hid_dim, 1, self.get_size(width))), requires_grad=True)\n",
    "        self.heads = heads\n",
    "        self.hid_dim = hid_dim\n",
    "        self.pe_type = pe_type\n",
    "        if pe_type == 'rotary':\n",
    "            self.mha = MultiHeadRotaryAttention(hid_dim, self.heads, is_2d=use_pe_2d)\n",
    "        else:\n",
    "            self.mha = nn.MultiheadAttention(hid_dim, self.heads)\n",
    "        self.trans_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hid_dim, nhead=self.heads, dim_feedforward=hid_dim, dropout=0.1, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        # output layer\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hid_dim, hid_dim // 4),\n",
    "            nn.Linear(hid_dim // 4, num_classes, bias=False)\n",
    "        )\n",
    "\n",
    "    def generate_positional_encoding(self, seq_len):\n",
    "        hid_dim = self.hid_dim\n",
    "        # Initialize the positional encoding matrix\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hid_dim, 2).float() * (-math.log(10000.0) / hid_dim))\n",
    "        pe = torch.zeros(seq_len, hid_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "    @staticmethod\n",
    "    def get_size(w, level=5):\n",
    "        for _ in range(level):\n",
    "            b = 1 if w % 2 == 1 else 0\n",
    "            w = int(w / 2 + b)\n",
    "        return int(w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # extract CNN features\n",
    "        x = self.features(x)  # [B, 192, 4, w]\n",
    "        x = self.cnn_proj(x)  # [B, 256, 4, w]\n",
    "        # Transformer encoder related operations\n",
    "        # Project and convert to channel-last\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        # Append class token\n",
    "        cls_token = self.cls_token.repeat_interleave(x.size(0), 0)\n",
    "        x = torch.cat([cls_token, x], 1)\n",
    "        if self.pe_type == 'rotary':\n",
    "            x = self.mha(x)\n",
    "        else:\n",
    "            pe = self.generate_positional_encoding(x.size(1)).to(x.device)\n",
    "            v, _ = self.mha[1](x + pe, x + pe, x + pe)\n",
    "            x = v + x\n",
    "        x = self.trans_encoder(x)[:, 0]\n",
    "        # projection head\n",
    "        x = self.cls_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T11:45:55.276343Z",
     "iopub.status.busy": "2024-04-07T11:45:55.275911Z",
     "iopub.status.idle": "2024-04-07T11:45:55.33779Z",
     "shell.execute_reply": "2024-04-07T11:45:55.336326Z",
     "shell.execute_reply.started": "2024-04-07T11:45:55.276309Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class InferHMSDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_path=None,\n",
    "            csv_path=None,\n",
    "            s_mean=None,\n",
    "            s_std=None,\n",
    "            cat_type='x',\n",
    "            df=None,\n",
    "            crop_size_sp=None,\n",
    "            crop_size_eeg=None,\n",
    "            slide_wd_ratio=0.5,  # -1 for no sliding window\n",
    "            is_train=False,\n",
    "            transform_sp=None,\n",
    "            transform_eeg=None,\n",
    "            duration=None,\n",
    "    ):\n",
    "        self.num_class = 6\n",
    "        # Read the main csv file\n",
    "        if df is None and dataset_path and csv_path:\n",
    "            self.df = pd.read_csv(os.path.join(dataset_path, csv_path))\n",
    "        elif df and (dataset_path is None and csv_path is None):\n",
    "            self.df = df\n",
    "        else:\n",
    "            raise ValueError('Either df or dataset_path and csv_path must be provided')\n",
    "\n",
    "        if s_std is None:\n",
    "            s_std = [2.3019, 2.2844, 2.2947, 2.3129]\n",
    "        if s_mean is None:\n",
    "            s_mean = [-.1119, -.129, -.1395, -.1689]\n",
    "        self.dataset_path = dataset_path\n",
    "        self.mean = s_mean\n",
    "        self.std = s_std\n",
    "        self.cat_type: str = cat_type\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.transform_sp = transform_sp if transform_sp else self.transform\n",
    "        self.transform_eeg = transform_eeg if transform_eeg else self.transform\n",
    "        if crop_size_sp is None:\n",
    "            crop_size_sp = [100, 256]\n",
    "        self.crop_size_sp = crop_size_sp\n",
    "        if crop_size_eeg is None:\n",
    "            crop_size_eeg = [100, 256]\n",
    "        self.crops_size_eeg = crop_size_eeg\n",
    "        self.slide_wd_ratio = slide_wd_ratio\n",
    "        self.base_dir = 'train' if is_train else 'test'\n",
    "        self.is_train = is_train\n",
    "        self.sensor_names = ['LL', 'RL', 'RP', 'LP']\n",
    "        self.eeg_feat = [['Fp1', 'F7', 'T3', 'T5', 'O1'],\n",
    "                         ['Fp1', 'F3', 'C3', 'P3', 'O1'],\n",
    "                         ['Fp2', 'F8', 'T4', 'T6', 'O2'],\n",
    "                         ['Fp2', 'F4', 'C4', 'P4', 'O2']]\n",
    "        self.eeg_sample_duration = duration if duration is not None else [10, 15, 30, 45]\n",
    "        self.spec_sample_duration = [600]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def log(self, kwargs: dict):\n",
    "        import mlflow\n",
    "        prefix = 'valid' if not self.is_train else 'train'\n",
    "        transform_sp_list = [t.__class__.__name__ for t in self.transform_sp.transforms]\n",
    "        transform_eeg_list = [t.__class__.__name__ for t in self.transform_eeg.transforms]\n",
    "        kwargs = {f'{prefix}-{k}': v for k, v in kwargs.items()}\n",
    "        mlflow.log_params({\n",
    "            f'{prefix}-mean': self.mean,\n",
    "            f'{prefix}-std': self.std,\n",
    "            f'{prefix}-transform_sp': transform_sp_list,\n",
    "            f'{prefix}-transform_eeg': transform_eeg_list,\n",
    "            **kwargs\n",
    "        })\n",
    "\n",
    "    def norm_img(self, img: np.ndarray, idx: int) -> np.ndarray:\n",
    "        \"\"\" Input image should be channel-last image \"\"\"\n",
    "        # std_img = (img - self.mean[idx]) / self.std[idx]\n",
    "        eps = 1e-6\n",
    "        std_img = (img - np.nanmean(img)) / (np.nanstd(img) + eps)\n",
    "        return std_img\n",
    "\n",
    "    def read_eeg(self, eeg_id, offset, h=50, w=256) -> list[list[np.ndarray]]:\n",
    "        parquet_path = f'{self.base_dir}_eegs/{eeg_id}.parquet'\n",
    "        parquet_path = os.path.join(self.dataset_path, parquet_path)\n",
    "        # LOAD MIDDLE 50 SECONDS OF EEG SERIES\n",
    "        eeg_ori = pd.read_parquet(parquet_path)\n",
    "        # Load multiple eeg from durations\n",
    "        d_imgs = []\n",
    "        for duration in self.eeg_sample_duration:\n",
    "            middle = int(offset + 25)\n",
    "            eeg = eeg_ori.iloc[(middle - duration // 2) * 200:(middle + duration // 2) * 200]\n",
    "            # VARIABLE TO HOLD SPECTROGRAM\n",
    "            img = np.zeros((h, w, 4), dtype='float32')\n",
    "            signals = []\n",
    "            for k in range(4):\n",
    "                COLS = self.eeg_feat[k]\n",
    "                for kk in range(4):\n",
    "                    # COMPUTE PAIR DIFFERENCES\n",
    "                    x = eeg[COLS[kk]].values - eeg[COLS[kk + 1]].values\n",
    "                    # FILL NANS\n",
    "                    m = np.nanmean(x)\n",
    "                    m = 0 if np.isnan(m) else m\n",
    "                    if np.isnan(x).mean() < 1:\n",
    "                        x = np.nan_to_num(x, nan=m)\n",
    "                    else:\n",
    "                        x[:] = 0\n",
    "                    # DENOISE\n",
    "                    signals.append(x)\n",
    "                    # RAW SPECTROGRAM\n",
    "                    mel_spec = melspectrogram(y=x, sr=200, hop_length=len(x) // w, n_fft=1024, n_mels=h, fmin=0,\n",
    "                                              fmax=20, win_length=128)\n",
    "                    # LOG TRANSFORM\n",
    "                    width = (mel_spec.shape[1] // 32) * 32\n",
    "                    mel_spec_db = power_to_db(mel_spec, ref=np.max).astype(np.float32)[:, :width]\n",
    "                    # STANDARDIZE TO -1 TO 1\n",
    "                    mel_spec_db = (mel_spec_db + 40) / 40\n",
    "                    img[:, :, k] += mel_spec_db\n",
    "                # AVERAGE THE 4 MONTAGE DIFFERENCES\n",
    "                img[:, :, k] /= 4.0\n",
    "            d_imgs.append([img[:, :, i] for i in range(img.shape[-1])])\n",
    "        return d_imgs\n",
    "\n",
    "    def read_spectrogram(self, spectrogram_id, offset) -> list[np.ndarray]:\n",
    "        \"\"\" Read the given spectrogram and apply normalization \"\"\"\n",
    "        spectrogram_path = f'{self.base_dir}_spectrograms/{spectrogram_id}.parquet'\n",
    "        raw = pd.read_parquet(os.path.join(self.dataset_path, spectrogram_path)).fillna(0)\n",
    "        sensor_types = self.sensor_names\n",
    "        raw = raw.loc[(raw.time >= offset) & (raw.time < offset + 600)]\n",
    "        sensor_data = [list(raw.filter(like=s, axis=1)) for s in sensor_types]\n",
    "        sensor_data = [np.log1p(raw[s].T.values) for s in sensor_data]\n",
    "        sensor_data = self.norm_img(np.stack(sensor_data, 0), -1)\n",
    "        sensor_data = np.split(sensor_data, sensor_data.shape[0], 0)\n",
    "        sensor_data = [np.nan_to_num(s, nan=0)[0] for s in sensor_data]\n",
    "        return sensor_data\n",
    "\n",
    "    def cat_imgs(self, img_list: list[torch.Tensor], data_type: str):\n",
    "        cat_dim = 'ch,x,y'.split(',').index(self.cat_type)\n",
    "        if data_type == 'eeg':\n",
    "            n = len(img_list) // len(self.eeg_sample_duration)\n",
    "            img_list = [img_list[i:i + n] for i in range(0, len(img_list), n)]\n",
    "        else:\n",
    "            img_list = [img_list]\n",
    "\n",
    "        result = []\n",
    "        for l in img_list:\n",
    "            l = torch.cat(l, cat_dim)\n",
    "            result.append(l)\n",
    "        if len(result) == 4:\n",
    "            result = [torch.cat(result[:2], -1), torch.cat(result[2:], -1)]\n",
    "        return torch.cat(result, 1)\n",
    "\n",
    "    def length_proces(self, t: torch.Tensor, data_type: str):\n",
    "        \"\"\" Process the length of the given tensor on the last dimension (image width) \"\"\"\n",
    "        assert data_type in ['spec', 'eeg'], f'data_type {data_type} not implemented'\n",
    "        base_length = self.crop_size_sp[1] if data_type == 'spec' else self.crops_size_eeg[1]\n",
    "        if data_type == 'eeg':\n",
    "            base_length *= 1 if len(self.eeg_sample_duration) <= 2 else len(self.eeg_sample_duration) // 2\n",
    "        min_sliding_len = base_length * self.slide_wd_ratio\n",
    "        # padding short image\n",
    "        if t.shape[-1] < base_length:\n",
    "            pad = base_length - t.shape[-1]\n",
    "            t = torch.nn.functional.pad(t, (0, pad), 'constant', 0)\n",
    "            return t.unsqueeze(0)\n",
    "        # sliding window\n",
    "        if self.slide_wd_ratio > 0 and t.shape[-1] > base_length:\n",
    "            t_list = []\n",
    "            while t.shape[-1] > min_sliding_len:\n",
    "                t_list.append(t[:, :, :base_length])\n",
    "                t = t[:, :, int(base_length * self.slide_wd_ratio):]\n",
    "            if t_list[-1].shape[-1] < base_length:\n",
    "                pad = base_length - t_list[-1].shape[-1]\n",
    "                t_list[-1] = torch.nn.functional.pad(t_list[-1], (0, pad), 'constant', 0)\n",
    "            t = torch.stack(t_list, 0)\n",
    "            return t\n",
    "        return t.unsqueeze(0)\n",
    "\n",
    "    def process_spectrogram(self, spectrogram: list[torch.Tensor], data_type: str):\n",
    "        spectrogram = self.cat_imgs(spectrogram, data_type)\n",
    "        spectrogram = self.length_proces(spectrogram, data_type)\n",
    "        return spectrogram\n",
    "\n",
    "    def get_spectrogram_by_id(self, spectrogram_id, offset: float) -> list[np.ndarray]:\n",
    "        spectrogram_images = self.read_spectrogram(spectrogram_id, offset)\n",
    "        return spectrogram_images\n",
    "\n",
    "    def get_eeg_by_id(self, eeg_id, offset) -> list[np.ndarray]:\n",
    "        eeg_images = self.read_eeg(eeg_id, offset)\n",
    "        eeg_image = [eeg for eeg_list in eeg_images for eeg in eeg_list]\n",
    "        return eeg_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        eeg_id = row['eeg_id']\n",
    "        spectrogram_id = row['spectrogram_id']\n",
    "        offset_eeg = 0\n",
    "        offset_spec = 0\n",
    "        eeg, spec = self.get_eeg_by_id(eeg_id, offset_eeg), self.get_spectrogram_by_id(spectrogram_id, offset_spec)\n",
    "        eeg, spec = [[trans(s) for s in ss] for trans, ss in zip([self.transform_eeg, self.transform_sp], [eeg, spec])]\n",
    "        return self.process_spectrogram(eeg, 'eeg'), self.process_spectrogram(spec, 'spec'), eeg_id\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        transposed_batch = list(zip(*batch))\n",
    "        eeg, spec, eeg_id = transposed_batch\n",
    "        data = []\n",
    "        for sidx, s in enumerate(spec):\n",
    "            bidx = s.size()[0]\n",
    "            e = eeg[sidx] * torch.ones((bidx, 1, 1, 1))\n",
    "            data.append(torch.cat([s, e], -1))\n",
    "        return data, eeg_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T11:37:34.754947Z",
     "iopub.status.busy": "2024-04-07T11:37:34.754562Z",
     "iopub.status.idle": "2024-04-07T11:37:34.770964Z",
     "shell.execute_reply": "2024-04-07T11:37:34.769583Z",
     "shell.execute_reply.started": "2024-04-07T11:37:34.754914Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_ema_model(checkpoint_path, model, device):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['ema'])\n",
    "\n",
    "def sliding_pred(imgs: torch.Tensor, model: torch.nn.Module, aggregate_method):\n",
    "    imgs = imgs.split(1, 0)\n",
    "    preds = [model(img) for img in imgs]\n",
    "    if aggregate_method == 'max':\n",
    "        preds = torch.stack(preds).max(0).values\n",
    "    elif aggregate_method == 'mean':\n",
    "        preds = torch.stack(preds).mean(0)\n",
    "    else:\n",
    "        raise NotImplementedError(f'aggregate_method: {aggregate_method} not implemented')\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T11:46:04.548343Z",
     "iopub.status.busy": "2024-04-07T11:46:04.547885Z",
     "iopub.status.idle": "2024-04-07T11:46:04.562761Z",
     "shell.execute_reply": "2024-04-07T11:46:04.561471Z",
     "shell.execute_reply.started": "2024-04-07T11:46:04.548309Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataset = InferHMSDataset(\n",
    "    dataset_path='../input/hms-harmful-brain-activity-classification',\n",
    "    csv_path='test.csv',\n",
    "    cat_type='x',\n",
    "    is_train=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T11:42:28.70502Z",
     "iopub.status.busy": "2024-04-07T11:42:28.704568Z",
     "iopub.status.idle": "2024-04-07T11:42:32.589441Z",
     "shell.execute_reply": "2024-04-07T11:42:32.588063Z",
     "shell.execute_reply.started": "2024-04-07T11:42:28.704984Z"
    }
   },
   "outputs": [],
   "source": [
    "kwargs = [\n",
    "    {'w': \n",
    "    '/kaggle/input/hms-baseline-efficient-b0/pytorch/efficientb0-wo-pl/1/f0.pt', 'cnn_type': 'b0'},\n",
    "    {'w': \n",
    "    '/kaggle/input/hms-baseline-efficient-b0/pytorch/efficientb0-wo-pl/1/f2.pt', 'cnn_type': 'b0'},\n",
    "    {'w': \n",
    "    '/kaggle/input/hms-baseline-efficient-b0/pytorch/efficientb0-wo-pl/1/f3.pt', 'cnn_type': 'b0'},\n",
    "    {'w': \n",
    "    '/kaggle/input/hms-baseline-efficient-b0/pytorch/mobilenetv3/1/f0.pt', 'cnn_type': 'mobilenet'},\n",
    "    {'w': \n",
    "    '/kaggle/input/hms-baseline-efficient-b0/pytorch/mobilenetv3/1/f2.pt', 'cnn_type': 'mobilenet'},\n",
    "    {'w': \n",
    "    '/kaggle/input/hms-baseline-efficient-b0/pytorch/mobilenetv3/1/f3.pt', 'cnn_type': 'mobilenet'},\n",
    "]\n",
    "\n",
    "model_weights = [1.2, 1.2, 1.2, 1, 1, 1]\n",
    "ensemble_models = []\n",
    "for kwarg in kwargs:\n",
    "    w_path = kwarg['w']\n",
    "    del kwarg['w']\n",
    "    net = EfficientNet(in_ch=1, width=256 + 256*2, weights=None, use_attention=False, **kwarg)\n",
    "    load_ema_model(w_path, net, device)\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    ensemble_models.append(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T11:46:07.63391Z",
     "iopub.status.busy": "2024-04-07T11:46:07.633451Z",
     "iopub.status.idle": "2024-04-07T11:46:12.848219Z",
     "shell.execute_reply": "2024-04-07T11:46:12.84659Z",
     "shell.execute_reply.started": "2024-04-07T11:46:07.633875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Open output file\n",
    "with open('submission_david_lb31.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow('eeg_id,seizure_vote,lpd_vote,gpd_vote,lrda_vote,grda_vote,other_vote'.split(','))\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=4, collate_fn=dataset.collate_fn)\n",
    "    dataset_iter = tqdm(data_loader, total=len(dataset), desc='Infering')\n",
    "    for data in dataset_iter:\n",
    "        img_tensor, eeg_id = data\n",
    "        with torch.no_grad():\n",
    "            outputs = []\n",
    "            for x in img_tensor:\n",
    "                pred_m = None\n",
    "                for model, w in zip(ensemble_models, model_weights):\n",
    "                    pred = sliding_pred(x.to(device), model, 'mean')\n",
    "                    pred_prob = torch.nn.functional.softmax(pred, dim=1)[0]\n",
    "                    pred_m = pred_prob * w if pred_m is None else pred_m + pred_prob * w\n",
    "                pred_m = pred_m / sum(model_weights)\n",
    "                outputs.append(pred_m)\n",
    "                \n",
    "        for oidx, output in enumerate(outputs):\n",
    "            probability = output.cpu().tolist()\n",
    "            content = [eeg_id[oidx].item()] + probability\n",
    "            writer.writerow(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T11:46:33.957882Z",
     "iopub.status.busy": "2024-04-07T11:46:33.957381Z",
     "iopub.status.idle": "2024-04-07T11:46:33.98979Z",
     "shell.execute_reply": "2024-04-07T11:46:33.988547Z",
     "shell.execute_reply.started": "2024-04-07T11:46:33.957832Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv('submission_david_lb31.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7469972,
     "sourceId": 59093,
     "sourceType": "competition"
    },
    {
     "datasetId": 4739216,
     "sourceId": 8038735,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 22685,
     "sourceId": 26929,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 22688,
     "sourceId": 26933,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
