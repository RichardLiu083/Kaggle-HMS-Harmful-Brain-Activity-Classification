{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T12:24:28.053416Z",
     "iopub.status.busy": "2024-03-28T12:24:28.053142Z",
     "iopub.status.idle": "2024-03-28T12:24:42.358129Z",
     "shell.execute_reply": "2024-03-28T12:24:42.35712Z",
     "shell.execute_reply.started": "2024-03-28T12:24:28.053392Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements:\n",
    "I am grateful to colleagues in the Kaggle community who shared valuable insights. The following notebooks were particularly enlightening:\n",
    "* Thank you to @pcjimmmy for the great insight on data separation -  [link](https://www.kaggle.com/code/pcjimmmy/patient-variation-eda)\n",
    "* Thank you to @cdeotte for the wonderful starter that helped me a lot in the competition - [link](https://www.kaggle.com/code/cdeotte/efficientnetb0-starter-lb-0-43)\n",
    "* Thank you to @seanbearden for the enlightening notebook regarding data separation - [link](https://www.kaggle.com/code/seanbearden/effnetb0-2-pop-model-train-twice-lb-0-39)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T12:24:42.360955Z",
     "iopub.status.busy": "2024-03-28T12:24:42.359968Z",
     "iopub.status.idle": "2024-03-28T12:24:42.374425Z",
     "shell.execute_reply": "2024-03-28T12:24:42.365826Z",
     "shell.execute_reply.started": "2024-03-28T12:24:42.360901Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set to True for inference only, False for training\n",
    "ONLY_INFERENCE = True\n",
    "\n",
    "# Configuration for model training\n",
    "FOLDS = 5\n",
    "EPOCHS = 4\n",
    "BATCH = 32\n",
    "NAME = 'None'\n",
    "\n",
    "SPEC_SIZE  = (512, 512, 3)\n",
    "CLASSES = [\"seizure_vote\", \"lpd_vote\", \"gpd_vote\", \"lrda_vote\", \"grda_vote\", \"other_vote\"]\n",
    "N_CLASSES = len(CLASSES)\n",
    "TARGETS = CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T12:24:42.382034Z",
     "iopub.status.busy": "2024-03-28T12:24:42.381097Z",
     "iopub.status.idle": "2024-03-28T12:24:56.335123Z",
     "shell.execute_reply": "2024-03-28T12:24:56.334072Z",
     "shell.execute_reply.started": "2024-03-28T12:24:42.381997Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install --no-index --find-links=/kaggle/input/tf-efficientnet-whl-files /kaggle/input/tf-efficientnet-whl-files/efficientnet-1.1.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T12:24:56.336866Z",
     "iopub.status.busy": "2024-03-28T12:24:56.33657Z",
     "iopub.status.idle": "2024-03-28T12:24:57.081875Z",
     "shell.execute_reply": "2024-03-28T12:24:57.080878Z",
     "shell.execute_reply.started": "2024-03-28T12:24:56.336839Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from tensorflow.keras import backend as K\n",
    "from tqdm import tqdm \n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "from scipy.signal import spectrogram as spectrogram_np\n",
    "\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "sys.path.append(f'/kaggle/input/kaggle-kl-div')\n",
    "from kaggle_kl_div import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T12:24:57.083895Z",
     "iopub.status.busy": "2024-03-28T12:24:57.083147Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!nvidia-smi\n",
    "\n",
    "# Installation of RAPIDS to Use cuSignal\n",
    "!cp ../input/rapids/rapids.0.17.0 /opt/conda/envs/rapids.tar.gz\n",
    "!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\n",
    "!rm /opt/conda/envs/rapids.tar.gz\n",
    "\n",
    "sys.path += [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"]\n",
    "sys.path += [\"/opt/conda/envs/rapids/lib/python3.7\"]\n",
    "sys.path += [\"/opt/conda/envs/rapids/lib\"]\n",
    "!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/\n",
    "\n",
    "import cupy as cp\n",
    "import cusignal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the visible CUDA devices\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "# Set the strategy for using GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus) <= 1:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    print(f'Using {len(gpus)} GPU')\n",
    "else:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f'Using {len(gpus)} GPUs')\n",
    "\n",
    "# Configure memory growth\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Enable or disable mixed precision\n",
    "MIX = True\n",
    "if MIX:\n",
    "    tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "    print('Mixed precision enabled')\n",
    "else:\n",
    "    print('Using full precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to set random seed for reproducibility\n",
    "def set_random_seed(seed: int = 42, deterministic: bool = False):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    if deterministic:\n",
    "        os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    else:\n",
    "        os.environ.pop('TF_DETERMINISTIC_OPS', None)\n",
    "\n",
    "# Set a deterministic behavior\n",
    "set_random_seed(deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data():\n",
    "    # Read the dataset\n",
    "    df = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\n",
    "    \n",
    "    # Create a new identifier combining multiple columns\n",
    "    id_cols = ['eeg_id', 'spectrogram_id', 'seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
    "    df['new_id'] = df[id_cols].astype(str).agg('_'.join, axis=1)\n",
    "    \n",
    "    # Calculate the sum of votes for each class\n",
    "    df['sum_votes'] = df[CLASSES].sum(axis=1)\n",
    "    \n",
    "    # Group the data by the new identifier and aggregate various features\n",
    "    agg_functions = {\n",
    "        'eeg_id': 'first',\n",
    "        'eeg_label_offset_seconds': ['min', 'max'],\n",
    "        'spectrogram_label_offset_seconds': ['min', 'max'],\n",
    "        'spectrogram_id': 'first',\n",
    "        'patient_id': 'first',\n",
    "        'expert_consensus': 'first',\n",
    "        **{col: 'sum' for col in CLASSES},\n",
    "        'sum_votes': 'mean',\n",
    "    }\n",
    "    grouped_df = df.groupby('new_id').agg(agg_functions).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns and adjust column names\n",
    "    grouped_df.columns = [f\"{col[0]}_{col[1]}\" if col[1] else col[0] for col in grouped_df.columns]\n",
    "    grouped_df.columns = grouped_df.columns.str.replace('_first', '').str.replace('_sum', '').str.replace('_mean', '')\n",
    "    \n",
    "    # Normalize the class columns\n",
    "    y_data = grouped_df[CLASSES].values\n",
    "    y_data_normalized = y_data / y_data.sum(axis=1, keepdims=True)\n",
    "    grouped_df[CLASSES] = y_data_normalized\n",
    "\n",
    "    # Split the dataset into high and low quality based on the sum of votes\n",
    "    high_quality_df = grouped_df[grouped_df['sum_votes'] >= 10].reset_index(drop=True)\n",
    "    low_quality_df = grouped_df[(grouped_df['sum_votes'] < 10) & (grouped_df['sum_votes'] >= 0)].reset_index(drop=True)\n",
    "\n",
    "    return high_quality_df, low_quality_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, data, batch_size=32, shuffle=False, mode='train'):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.mode = mode\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch.\"\"\"\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data.\"\"\"\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        X, y = self.__data_generation(indexes)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch.\"\"\"\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        \"\"\"Generates data containing batch_size samples.\"\"\"\n",
    "        # Initialization\n",
    "        X = np.zeros((len(indexes), *SPEC_SIZE), dtype='float32')\n",
    "        y = np.zeros((len(indexes), len(CLASSES)), dtype='float32')\n",
    "\n",
    "        # Generate data\n",
    "        for j, i in enumerate(indexes):\n",
    "            row = self.data.iloc[i]\n",
    "            eeg_id = row['eeg_id']\n",
    "            spec_offset = int(row['spectrogram_label_offset_seconds_min'])\n",
    "            eeg_offset = int(row['eeg_label_offset_seconds_min'])\n",
    "            file_path = f'/kaggle/input/3-diff-time-specs-hms/images/{eeg_id}_{spec_offset}_{eeg_offset}.npz'\n",
    "            data = np.load(file_path)\n",
    "            eeg_data = data['final_image']\n",
    "            eeg_data_expanded = np.repeat(eeg_data[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "            X[j] = eeg_data_expanded\n",
    "            if self.mode != 'test':\n",
    "                y[j] = row[CLASSES]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def lrfn(epoch):\n",
    "    lr_schedule = [1e-3, 1e-3, 1e-3, 1e-4, 1e-4, 1e-4, 1e-5, 1e-5, 1e-5]\n",
    "    return lr_schedule[epoch]\n",
    "\n",
    "# Define the learning rate scheduler callback\n",
    "LR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n",
    "\n",
    "def build_EfficientNetB0(input_shape=(512, 512, 3), num_classes=6):\n",
    "    inp = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    base_model = efn.EfficientNetB0(include_top=False, weights=None, input_shape=None)\n",
    "    base_model.load_weights(f'/kaggle/input/tf-efficientnet-imagenet-weights/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5')\n",
    "\n",
    "    # OUTPUT\n",
    "    x = base_model(inp)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(num_classes,activation='softmax', dtype='float32')(x)\n",
    "\n",
    "    # COMPILE MODEL\n",
    "    model = tf.keras.Model(inputs=inp, outputs=x)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
    "    loss = tf.keras.losses.KLDivergence()\n",
    "\n",
    "    model.compile(loss=loss, optimizer = opt)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross_validate_model - Label Refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def cross_validate_model(train_data, train_data_2, folds, random_seed, targets, nome_modelo):\n",
    "    inicio = time.time()\n",
    "    path_model = f'MLP_Model{nome_modelo}'\n",
    "    if not os.path.exists(path_model):\n",
    "        os.makedirs(path_model)\n",
    "\n",
    "    all_oof = []\n",
    "    all_oof2 = []\n",
    "    all_true = []\n",
    "    models = []\n",
    "    score_list = []\n",
    "    \n",
    "    # Separating the data to iterate over both dataframes simultaneously\n",
    "    gkf = StratifiedGroupKFold(n_splits=folds, shuffle=True, random_state=random_seed)\n",
    "    splits1 = list(gkf.split(train_data, train_data[[\"expert_consensus\"]], train_data[\"patient_id\"]))\n",
    "    splits2 = list(gkf.split(train_data_2, train_data_2[[\"expert_consensus\"]], train_data_2[\"patient_id\"]))\n",
    "\n",
    "    # Iterate over folds in parallel\n",
    "    for i, ((train_index, valid_index), (train_index2, valid_index2)) in enumerate(zip(splits1, splits2)):\n",
    "        \n",
    "        # Copy the dataframes to avoid leaks\n",
    "        train_data_ = train_data.copy()\n",
    "        train_data_2_ = train_data_2.copy()\n",
    "        set_random_seed(random_seed, deterministic=True)\n",
    "        \n",
    "        # Start folding\n",
    "        print('#' * 25)\n",
    "        print(f'### Fold {i + 1}')\n",
    "        print(f'### train size 1 {len(train_index)}, valid size {len(valid_index)}')\n",
    "        print(f'### train size 2 {len(train_index2)}, valid size {len(valid_index2)}')\n",
    "        print('#' * 25)\n",
    "\n",
    "        ### --------------------------- Performs model 1 training -------------- --------------------------- ###\n",
    "        K.clear_session()\n",
    "        train_gen = DataGenerator(train_data_.iloc[train_index], shuffle=True, batch_size=BATCH)\n",
    "        valid_gen = DataGenerator(train_data_.iloc[valid_index], shuffle=False, batch_size=(BATCH*2), mode='valid')\n",
    "        model = build_EfficientNetB0(input_shape=(512, 512, 3), num_classes=6)\n",
    "        history = model.fit(train_gen, verbose=2, validation_data=valid_gen, epochs=EPOCHS, callbacks=[LR])\n",
    "\n",
    "        # Model training result 1\n",
    "        train_loss = history.history['loss'][-1]  \n",
    "        valid_loss = history.history['val_loss'][-1]\n",
    "        print(f'train_loss 1 {train_loss} valid_loss 1 {valid_loss}')\n",
    "        score_list.append((train_loss, valid_loss))\n",
    "\n",
    "        \n",
    "        ### --------------------------- creation of pseudo labels ---------------- ------------------------- ###\n",
    "        # pseudo labels for low quality data\n",
    "        train_2_index_total_gen = DataGenerator(train_data_2_.iloc[train_index2], shuffle=False, batch_size=BATCH)\n",
    "        pseudo_labels_2 = model.predict(train_2_index_total_gen, verbose=2)\n",
    "        # Refinement of low quality labels\n",
    "        train_data_2_.loc[train_index2, TARGETS] /= 2\n",
    "        train_data_2_.loc[train_index2, TARGETS] += pseudo_labels_2 / 2\n",
    "\n",
    "        # pseudo labels for high quality data (50% of data)\n",
    "        train_data_3_ = train_data_\n",
    "        train_3_index_total_gen = DataGenerator(train_data_3_.iloc[train_index], shuffle=False, batch_size=BATCH)\n",
    "        pseudo_labels_3 = model.predict(train_3_index_total_gen, verbose=2)\n",
    "        # Refinement of high quality labels\n",
    "        train_data_3_.loc[train_index, TARGETS] /= 2\n",
    "        train_data_3_.loc[train_index, TARGETS] += pseudo_labels_3 / 2\n",
    "\n",
    "        ### --------------------------- Creation of the data generator for the refined labels model --------- -------------------------------- ###\n",
    "        # Low quality data\n",
    "        np.random.shuffle(train_index)\n",
    "        np.random.shuffle(valid_index)\n",
    "        sixty_percent_length = int(0.5 * len(train_data_3_))\n",
    "        train_index_60 = train_index[:int(sixty_percent_length * len(train_index) / len(train_data_3_))]\n",
    "        valid_index_60 = valid_index[:int(sixty_percent_length * len(valid_index) / len(train_data_3_))]\n",
    "        train_gen_2 = DataGenerator(pd.concat([train_data_3_.iloc[train_index_60], train_data_2_.iloc[train_index2]]), shuffle=True, batch_size=BATCH)\n",
    "        valid_gen_2 = DataGenerator(pd.concat([train_data_3_.iloc[valid_index_60], train_data_2_.iloc[valid_index2]]), shuffle=False, batch_size=BATCH*2, mode='valid')\n",
    "        # Rebuild the high quality data generator with 50% of the labels refined\n",
    "        train_gen = DataGenerator(train_data_.iloc[train_index], shuffle=True, batch_size=BATCH)\n",
    "        valid_gen = DataGenerator(train_data_.iloc[valid_index], shuffle=False, batch_size=(BATCH*2), mode='valid')\n",
    "        \n",
    "        ### --------------------------- Model 2 training and finetunning -------------- --------------------------- ###\n",
    "        K.clear_session()\n",
    "        new_model = build_EfficientNetB0(input_shape=(512, 512, 3), num_classes=6)\n",
    "        # Training with the refined low-quality data\n",
    "        history = new_model.fit(train_gen_2, verbose=2, validation_data=valid_gen_2, epochs=EPOCHS, callbacks=[LR])\n",
    "        # Finetuning with refined high-quality data\n",
    "        history = new_model.fit(train_gen, verbose=2, validation_data=valid_gen, epochs=EPOCHS, callbacks=[LR])\n",
    "        new_model.save_weights(f'{path_model}/MLP_fold{i}.weights.h5')\n",
    "        models.append(new_model)\n",
    "\n",
    "        # Model 2 training result\n",
    "        train_loss = history.history['loss'][-1]  # Valor da perda do último epoch de treinamento\n",
    "        valid_loss = history.history['val_loss'][-1]  # Valor da perda do último epoch de validação\n",
    "        print(f'train_loss 2 {train_loss} valid_loss 2 {valid_loss}')\n",
    "        score_list.append((train_loss, valid_loss))\n",
    "\n",
    "\n",
    "        # MLP OOF\n",
    "        oof = new_model.predict(valid_gen, verbose=2)\n",
    "        all_oof.append(oof)\n",
    "        all_true.append(train_data.iloc[valid_index][TARGETS].values)\n",
    "\n",
    "        # TRAIN MEAN OOF\n",
    "        y_train = train_data.iloc[train_index][targets].values\n",
    "        y_valid = train_data.iloc[valid_index][targets].values\n",
    "        oof = y_valid.copy()\n",
    "        for j in range(6):\n",
    "            oof[:,j] = y_train[:,j].mean()\n",
    "        oof = oof / oof.sum(axis=1,keepdims=True)\n",
    "        all_oof2.append(oof)\n",
    "\n",
    "        del model, new_model, train_gen, valid_gen, train_2_index_total_gen, train_gen_2, valid_gen_2, oof, y_train, y_valid, train_index, valid_index\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        if i==folds-1: break\n",
    "\n",
    "    all_oof = np.concatenate(all_oof)\n",
    "    all_oof2 = np.concatenate(all_oof2)\n",
    "    all_true = np.concatenate(all_true)\n",
    "\n",
    "    oof = pd.DataFrame(all_oof.copy())\n",
    "    oof['id'] = np.arange(len(oof))\n",
    "\n",
    "    true = pd.DataFrame(all_true.copy())\n",
    "    true['id'] = np.arange(len(true))\n",
    "\n",
    "    cv = score(solution=true, submission=oof, row_id_column_name='id')\n",
    "    fim = time.time()\n",
    "    tempo_execucao = fim - inicio\n",
    "    print(f'{nome_modelo} CV Score with EEG Spectrograms ={cv} tempo: {tempo_execucao}')\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    score_array = np.array(score_list)\n",
    "    std_dev = np.std(score_array, axis=0)\n",
    "    std_dev = std_dev.tolist()\n",
    "\n",
    "    return cv, tempo_execucao, all_oof, all_oof2, all_true, models, score_list, std_dev, path_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if not ONLY_INFERENCE:\n",
    "    high_quality_df, low_quality_df = create_train_data()\n",
    "    result, tempo_execucao, all_oof, all_oof2, all_true, models, score_list, std_dev, path_model = cross_validate_model(high_quality_df, low_quality_df, FOLDS, 42, CLASSES, NAME)\n",
    "    print(f'Result cv V1 final {result}{tempo_execucao} {score_list} {std_dev}')\n",
    "    display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_spectrogram_with_cusignal(eeg_data, eeg_id, start, duration= 50,\n",
    "                                    low_cut_freq = 0.7, high_cut_freq = 20, order_band = 5,\n",
    "                                    spec_size_freq = 267, spec_size_time = 30,\n",
    "                                    nperseg_ = 1500, noverlap_ = 1483, nfft_ = 2750,\n",
    "                                    sigma_gaussian = 0.7, \n",
    "                                    mean_montage_names = 4):\n",
    "    \n",
    "    electrode_names = ['LL', 'RL', 'LP', 'RP']\n",
    "\n",
    "    electrode_pairs = [\n",
    "        ['Fp1', 'F7', 'T3', 'T5', 'O1'],\n",
    "        ['Fp2', 'F8', 'T4', 'T6', 'O2'],\n",
    "        ['Fp1', 'F3', 'C3', 'P3', 'O1'],\n",
    "        ['Fp2', 'F4', 'C4', 'P4', 'O2']\n",
    "    ]\n",
    "    \n",
    "    # Filter specifications\n",
    "    nyquist_freq = 0.5 * 200\n",
    "    low_cut_freq_normalized = low_cut_freq / nyquist_freq\n",
    "    high_cut_freq_normalized = high_cut_freq / nyquist_freq\n",
    "\n",
    "    # Bandpass and notch filter\n",
    "    bandpass_coefficients = butter(order_band, [low_cut_freq_normalized, high_cut_freq_normalized], btype='band')\n",
    "    notch_coefficients = iirnotch(w0=60, Q=30, fs=200)\n",
    "    \n",
    "    spec_size = duration * 200\n",
    "    start = start * 200\n",
    "    real_start = start + (10_000//2) - (spec_size//2)\n",
    "    eeg_data = eeg_data.iloc[real_start:real_start+spec_size]\n",
    "    \n",
    "    \n",
    "    # Spectrogram parameters\n",
    "    fs = 200\n",
    "    nperseg = nperseg_\n",
    "    noverlap = noverlap_\n",
    "    nfft = nfft_\n",
    "    \n",
    "    if spec_size_freq <=0 or spec_size_time <=0:\n",
    "        frequencias_size = int((nfft // 2)/5.15198)+1\n",
    "        segmentos = int((spec_size - noverlap) / (nperseg - noverlap)) \n",
    "    else:\n",
    "        frequencias_size = spec_size_freq\n",
    "        segmentos = spec_size_time\n",
    "        \n",
    "    spectrogram = cp.zeros((frequencias_size, segmentos, 4), dtype='float32')\n",
    "    \n",
    "    processed_eeg = {}\n",
    "\n",
    "    for i, name in enumerate(electrode_names):\n",
    "        cols = electrode_pairs[i]\n",
    "        processed_eeg[name] = np.zeros(spec_size)\n",
    "        for j in range(4):\n",
    "            # Compute differential signals\n",
    "            signal = cp.array(eeg_data[cols[j]].values - eeg_data[cols[j+1]].values)\n",
    "\n",
    "            # Handle NaNs\n",
    "            mean_signal = cp.nanmean(signal)\n",
    "            signal = cp.nan_to_num(signal, nan=mean_signal) if cp.isnan(signal).mean() < 1 else cp.zeros_like(signal)\n",
    "            \n",
    "\n",
    "            # Filter bandpass and notch\n",
    "            signal_filtered = filtfilt(*notch_coefficients, signal.get())\n",
    "            signal_filtered = filtfilt(*bandpass_coefficients, signal_filtered)\n",
    "            signal = cp.asarray(signal_filtered)\n",
    "            \n",
    "            frequencies, times, Sxx = cusignal.spectrogram(signal, fs, nperseg=nperseg, noverlap=noverlap, nfft=nfft)\n",
    "\n",
    "            # Filter frequency range\n",
    "            valid_freqs = (frequencies >= 0.59) & (frequencies <= 20)\n",
    "            frequencies_filtered = frequencies[valid_freqs]\n",
    "            Sxx_filtered = Sxx[valid_freqs, :]\n",
    "\n",
    "            # Logarithmic transformation and normalization using Cupy\n",
    "            spectrogram_slice = cp.clip(Sxx_filtered, cp.exp(-4), cp.exp(6))\n",
    "            spectrogram_slice = cp.log10(spectrogram_slice)\n",
    "\n",
    "            normalization_epsilon = 1e-6\n",
    "            mean = spectrogram_slice.mean(axis=(0, 1), keepdims=True)\n",
    "            std = spectrogram_slice.std(axis=(0, 1), keepdims=True)\n",
    "            spectrogram_slice = (spectrogram_slice - mean) / (std + normalization_epsilon)\n",
    "            \n",
    "            spectrogram[:, :, i] += spectrogram_slice\n",
    "            processed_eeg[f'{cols[j]}_{cols[j+1]}'] = signal.get()\n",
    "            processed_eeg[name] += signal.get()\n",
    "        \n",
    "        # AVERAGE THE 4 MONTAGE DIFFERENCES\n",
    "        if mean_montage_names > 0:\n",
    "            spectrogram[:,:,i] /= mean_montage_names\n",
    "\n",
    "    # Convert to NumPy and apply Gaussian filter\n",
    "    spectrogram_np = cp.asnumpy(spectrogram)\n",
    "    if sigma_gaussian > 0.0:\n",
    "        spectrogram_np = gaussian_filter(spectrogram_np, sigma=sigma_gaussian)\n",
    "\n",
    "    # Filter EKG signal\n",
    "    ekg_signal_filtered = filtfilt(*notch_coefficients, eeg_data[\"EKG\"].values)\n",
    "    ekg_signal_filtered = filtfilt(*bandpass_coefficients, ekg_signal_filtered)\n",
    "    processed_eeg['EKG'] = np.array(ekg_signal_filtered)\n",
    "\n",
    "    return spectrogram_np, processed_eeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_spectogram_competition(spec_id, seconds_min):\n",
    "    spec = pd.read_parquet(f'/kaggle/input/hms-harmful-brain-activity-classification/test_spectrograms/{spec_id}.parquet')\n",
    "    inicio = (seconds_min) // 2\n",
    "    img = spec.fillna(0).values[:, 1:].T.astype(\"float32\")\n",
    "    img = img[:, inicio:inicio+300]\n",
    "    \n",
    "    # Log transform and normalize\n",
    "    img = np.clip(img, np.exp(-4), np.exp(6))\n",
    "    img = np.log(img)\n",
    "    eps = 1e-6\n",
    "    img_mean = img.mean()\n",
    "    img_std = img.std()\n",
    "    img = (img - img_mean) / (img_std + eps)\n",
    "    \n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "all_eegs2 = {}\n",
    "# Make sure the 'images' folder exists\n",
    "output_folder = 'imagens'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    \n",
    "test = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/test.csv')\n",
    "print('Test shape:',test.shape)\n",
    "print(test.head())\n",
    "\n",
    "# Creation of spectograms on the test base\n",
    "for i in tqdm(range(len(test)), desc=\"Processing EEGs\"):\n",
    "    row = test.iloc[i]\n",
    "    eeg_id = row['eeg_id']\n",
    "    spec_id = row['spectrogram_id']\n",
    "    seconds_min = 0\n",
    "    start_second = 0\n",
    "    eeg_data = pd.read_parquet(f'/kaggle/input/hms-harmful-brain-activity-classification/test_eegs/{eeg_id}.parquet')\n",
    "    eeg_new_key = eeg_id\n",
    "    image_50s, _ = create_spectrogram_with_cusignal(eeg_data=eeg_data, eeg_id=eeg_id, start=start_second, duration= 50,\n",
    "                                    low_cut_freq = 0.7, high_cut_freq = 20, order_band = 5,\n",
    "                                    spec_size_freq = 267, spec_size_time = 501,\n",
    "                                    nperseg_ = 1500, noverlap_ = 1483, nfft_ = 2750,\n",
    "                                    sigma_gaussian = 0.0, \n",
    "                                    mean_montage_names = 4)\n",
    "    image_10s, _ = create_spectrogram_with_cusignal(eeg_data=eeg_data, eeg_id=eeg_id, start=start_second, duration= 10,\n",
    "                                    low_cut_freq = 0.7, high_cut_freq = 20, order_band = 5,\n",
    "                                    spec_size_freq = 100, spec_size_time = 291,\n",
    "                                    nperseg_ = 260, noverlap_ = 254, nfft_ = 1030,\n",
    "                                    sigma_gaussian = 0.0, \n",
    "                                    mean_montage_names = 4)\n",
    "    image_10m = create_spectogram_competition(spec_id, seconds_min)\n",
    "    \n",
    "    imagem_final_unico_canal = np.zeros((1068, 501))\n",
    "    for j in range(4):\n",
    "        inicio = j * 267 \n",
    "        fim = inicio + 267\n",
    "        imagem_final_unico_canal[inicio:fim, :] = image_50s[:, :, j]\n",
    "        \n",
    "    \n",
    "    imagem_final_unico_canal2 = np.zeros((400, 291))\n",
    "    for n in range(4):\n",
    "        inicio = n * 100 \n",
    "        fim = inicio + 100\n",
    "        imagem_final_unico_canal2[inicio:fim, :] = image_10s[:, :, n]\n",
    "    \n",
    "    imagem_final_unico_canal_resized = cv2.resize(imagem_final_unico_canal, (400, 800), interpolation=cv2.INTER_AREA)\n",
    "    imagem_final_unico_canal2_resized = cv2.resize(imagem_final_unico_canal2, (300, 400), interpolation=cv2.INTER_AREA)\n",
    "    eeg_new_resized = cv2.resize(image_10m, (300, 400), interpolation=cv2.INTER_AREA)\n",
    "    imagem_final = np.zeros((800, 700), dtype=np.float32)\n",
    "    imagem_final[0:800, 0:400] = imagem_final_unico_canal_resized\n",
    "    imagem_final[0:400,400:700] = imagem_final_unico_canal2_resized\n",
    "    imagem_final[400:800, 400:700] = eeg_new_resized\n",
    "    imagem_final = imagem_final[::-1]\n",
    "    \n",
    "    imagem_final = cv2.resize(imagem_final, (512, 512), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    all_eegs2[eeg_new_key] = imagem_final\n",
    "    \n",
    "    if i ==0:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(imagem_final, cmap='jet')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        print(imagem_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DataGeneratorTest(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data, batch_size=32, shuffle=False, eegs={}, mode='train'):\n",
    "\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.eegs = eegs\n",
    "        self.mode = mode\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        ct = int( np.ceil( len(self.data) / self.batch_size ) )\n",
    "        return ct\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X, y = self.__data_generation(indexes)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange( len(self.data) )\n",
    "        if self.shuffle: np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples'\n",
    "\n",
    "        X = np.zeros((len(indexes),SPEC_SIZE[0],SPEC_SIZE[1],SPEC_SIZE[2]),dtype='float32')\n",
    "        y = np.zeros((len(indexes),6),dtype='float32')\n",
    "\n",
    "        for j,i in enumerate(indexes):\n",
    "            row = self.data.iloc[i]\n",
    "            eeg_data = self.eegs[row.eeg_id] \n",
    "            eeg_data_expanded = np.repeat(eeg_data[:, :, np.newaxis], 3, axis=2)\n",
    "            X[j,] = eeg_data_expanded\n",
    "            if self.mode!='test':\n",
    "                y[j] = row[CLASSES]\n",
    "\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFER MLP ON TEST\n",
    "preds = []\n",
    "model = build_EfficientNetB0((SPEC_SIZE[0],SPEC_SIZE[1],SPEC_SIZE[2]), 6)\n",
    "test_gen = DataGeneratorTest(test, shuffle=False, batch_size=BATCH, eegs=all_eegs2, mode='test')\n",
    "\n",
    "print('Inferring test... ',end='')\n",
    "for i in range(FOLDS):\n",
    "    print(f'fold {i+1}, ',end='')\n",
    "    model.load_weights(f'/kaggle/input/train-lf-hms/MLP_fold{i}.weights.h5')\n",
    "    pred = model.predict(test_gen, verbose=0)\n",
    "    preds.append(pred)\n",
    "pred = np.mean(preds,axis=0)\n",
    "print()\n",
    "print('Test preds shape',pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CREATE SUBMISSION.CSV\n",
    "from IPython.display import display\n",
    "\n",
    "sub = pd.DataFrame({'eeg_id':test.eeg_id.values})\n",
    "sub[TARGETS] = pred\n",
    "sub.to_csv('submission_lb31.csv',index=False)\n",
    "print('Submission shape',sub.shape)\n",
    "display( sub.head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SANITY CHECK TO CONFIRM PREDICTIONS SUM TO ONE\n",
    "print(sub.iloc[:,-6:].sum(axis=1))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7469972,
     "sourceId": 59093,
     "sourceType": "competition"
    },
    {
     "datasetId": 492658,
     "sourceId": 2378330,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4297749,
     "sourceId": 7392733,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4304475,
     "sourceId": 7402356,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4532886,
     "sourceId": 7754261,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4533761,
     "sourceId": 8028982,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 158958765,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
